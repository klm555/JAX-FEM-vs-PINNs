{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Portions of this code are adapted from:\n",
    "#   - https://github.com/TamaraGrossmann/FEM-vs-PINNs.git\n",
    "#   - Grossmann, T. G., Komorowska, U. J., Latz, J., & Schönlieb, C.-B. (2023).\n",
    "#     Can Physics-Informed Neural Networks beat the Finite Element Method?\n",
    "#     arXiv:2302.04107.\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for Google Colab)\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, flax, optax, time, pickle\n",
    "import os\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from functools import partial\n",
    "from pyDOE import lhs\n",
    "from typing import Sequence\n",
    "import json\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5543116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must set CUDA_VISIBLE_DEVICES before importing JAX or any other library that initializes GPUs.\n",
    "# Otherwise, the environment variable change might be ignored.\n",
    "# \"0, 1\": first two GPUs / \"\": no GPU (CPU instead)\n",
    "\n",
    "# Run on the first GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from jax.extend.backend import get_backend\n",
    "print(get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90993946",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d466d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_list = [[20, 20, 20, 1], [100, 100, 100, 1], [500, 500, 500, 1], [2500, 2500, 2500, 1]]\n",
    "\n",
    "# architecture_list = [[20, 20, 20, 1], [100, 100, 100, 1], [500, 500, 500, 1],\n",
    "#                      [20, 20, 20, 20, 1], [100, 100, 100, 100, 1], [500, 500, 500, 500, 1],]\n",
    "                     # [20, 20, 20, 20, 20, 1], [100, 100, 100, 100, 100, 1], [500, 500, 500, 500, 500, 1],\n",
    "                     # [20, 20, 20, 20, 20, 20, 1], [100, 100, 100, 100, 100, 100, 1], [500, 500, 500, 500, 500, 500, 1],\n",
    "                     # [20, 20, 20, 20, 20, 20, 20, 1], [100, 100, 100, 100, 100, 100, 100, 1]] # NN architecture list\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 50000 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af130ab8",
   "metadata": {},
   "source": [
    "# NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN architecture\n",
    "class PDESolution(flax.linen.Module): # inherit from Module class\n",
    "    features: Sequence[int] # dataclass (e.g. [10, 20, 1])\n",
    "\n",
    "    @flax.linen.compact # a decorator to define the model in more concise and readable way\n",
    "    def __call__(self, x): # __call__: makes an object callable, which enables you to use instances of the class like functions\n",
    "        for feature in self.features[:-1]:\n",
    "            x = flax.linen.tanh(flax.linen.Dense(feature)(x))\n",
    "        # Final Dense layer\n",
    "        x = flax.linen.Dense(self.features[-1])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e3586",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE residual\n",
    "@partial(jax.vmap, in_axes = (None, 0, 0, 0), out_axes = 0)\n",
    "@partial(jax.jit, static_argnums = (0,)) # decorator closest to the function is applied first\n",
    "def residual(u, t, x, y):\n",
    "    u_t = jax.jvp(u, (t, x, y), (1., 0., 0.))[1] # partial derivative w.r.t t = directional derivative along <1, 0>\n",
    "    u_xx = jax.hessian(u, argnums = 1)(t, x, y) # differentiate w.r.t argument 1(x)\n",
    "    u_yy = jax.hessian(u, argnums = 2)(t, x, y)\n",
    "    f = 10.0 * np.sin(np.pi*x) * np.sin(np.pi*y) * np.cos(2.0*np.pi*t) # time-varying source\n",
    "    return u_t - (u_xx + u_yy) - f\n",
    "\n",
    "# Inital condition\n",
    "@partial(jax.vmap, in_axes=(0, 0)) # vectorized over \"xs\" and \"ys\"\n",
    "def u_init(xs, ys):\n",
    "    return np.array([np.exp(-50.0*((xs-0.5)**2 + (ys-0.5)**2))])\n",
    "\n",
    "# Loss functionals\n",
    "@jax.jit\n",
    "def pde_residual(params, points):\n",
    "    return np.mean(residual(lambda t, x, y: model.apply(params, np.stack((t, x, y))), points[:, 0], points[:, 1], points[:, 2]) ** 2) # Mean Squared Error\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def init_residual(u_init, params, points):\n",
    "    lhs = model.apply(params, np.stack((np.zeros_like(points[:, 0]), points[:, 1], points[:, 2]), axis=1))\n",
    "    rhs = u_init(points[:, 1], points[:, 2])\n",
    "    return np.mean((lhs - rhs) ** 2)\n",
    "\n",
    "@jax.jit\n",
    "def dirichlet_residual_x(params, points):\n",
    "    return np.mean((model.apply(params, np.stack((points[:,0], np.zeros_like(points[:,1]), points[:,2]), axis=1))) ** 2)\n",
    "\n",
    "@jax.jit\n",
    "def dirichlet_residual_y(params, points):\n",
    "    return np.mean((model.apply(params, np.stack((points[:,0], points[:,1], np.zeros_like(points[:,2])), axis=1))) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d74a6",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b823ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Step\n",
    "@partial(jax.jit, static_argnums = (1,))\n",
    "def training_step_ini(params, opt, opt_state, key):\n",
    "    \"\"\"\n",
    "    In the PDE problem with initial condition, training NN with I.C. first can be helpful because\n",
    "    - it does not need so many training steps(epochs). (initial guess is already close to the solution)\n",
    "    - It stabilizes subsequent training, because the solution is already near the correct initial profile.\n",
    "\n",
    "    Args:\n",
    "        params: model parameters\n",
    "        opt: optimizer\n",
    "        opt_state: optimizer state\n",
    "        key: random key for sampling\n",
    "    \"\"\"\n",
    "    lb = np.array([0., 0., 0.]) # lower bound\n",
    "    ub = np.array([1., 1., 1.]) # upper bound\n",
    "    domain_points = lb + (ub - lb) * lhs(3, 5000) # latin hypercube sampling 20000 points within (ti, xi) ∈ [0, 0.05] × [0, 1]\n",
    "    boundary_points = lb + (ub - lb) * lhs(3, 100) # latin hypercube sampling 250 points within ti ∈ [0, 0.05]\n",
    "    init_points = lb[1:] + (ub[1:] - lb[1:]) * lhs(2, 100) # latin hypercube sampling 500 points within xi, yi ∈ [0, 1]\n",
    "\n",
    "    loss_init = init_residual(u_init, params, init_points)\n",
    "\n",
    "    loss_val, grad = jax.value_and_grad(lambda params: loss_init)(params)\n",
    "\n",
    "    update, opt_state = opt.update(grad, opt_state, params) # update using \"grad\"\n",
    "    params = optax.apply_updates(params, update) # apply updates to \"params\"\n",
    "    return params, opt_state, key, loss_val\n",
    "\n",
    "@partial(jax.jit, static_argnums = (1,))\n",
    "def training_step(params, opt, opt_state, key):\n",
    "    lb = np.array([0., 0., 0.])\n",
    "    ub = np.array([1., 1., 1.])\n",
    "    domain_points = lb + (ub - lb) * lhs(3, 5000) # latin hypercube sampling 20000 points within (ti, xi) ∈ [0, 0.05] × [0, 1]\n",
    "    boundary_points = lb + (ub - lb) * lhs(3, 100) # latin hypercube sampling 250 points within ti ∈ [0, 0.05]\n",
    "    init_points = lb[1:] + (ub[1:] - lb[1:]) * lhs(2, 100) # latin hypercube sampling 500 points within xi, yi ∈ [0, 1]\n",
    "\n",
    "    loss_pde = pde_residual(params, domain_points)\n",
    "    loss_init = init_residual(u_init, params, init_points)\n",
    "    loss_dirichlet_x = dirichlet_residual_x(params, boundary_points)\n",
    "    loss_dirichlet_y = dirichlet_residual_y(params, boundary_points)\n",
    "\n",
    "    loss_val, grad = jax.value_and_grad(lambda params: loss_pde + loss_init + loss_dirichlet_x + loss_dirichlet_y)(params)\n",
    "\n",
    "    update, opt_state = opt.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, update)\n",
    "    return params, opt_state, key, loss_val\n",
    "\n",
    "# Training loop\n",
    "def train_loop(params, adam, opt_state, key):\n",
    "    losses = []\n",
    "    for i in range(7000):\n",
    "        params, opt_state, key, loss_val = training_step_ini(params, adam, opt_state, key)\n",
    "        losses.append(loss_val.item())\n",
    "        if (i+1) % 2000 == 0: # print every 1000 epochs\n",
    "            print(f\"[Training with I.C. loss] Epoch {i + 1}: Loss = {loss_val.item()}\")\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        params, opt_state, key, loss_val = training_step(params, adam, opt_state, key)\n",
    "        losses.append(loss_val.item())\n",
    "        if (j+1) % 10000 == 0:\n",
    "            print(f\"[Training with total loss] Epoch {j + 1}: Loss = {loss_val.item()}\")\n",
    "    return losses, params, opt_state, key, loss_val # return final values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc034fa7",
   "metadata": {},
   "source": [
    "# Helper Functions for L-BFGS Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS requires the parameters to be a single flattened array!\n",
    "def concat_params(params): # flatten the parameters\n",
    "    params, tree = jax.tree_util.tree_flatten(params) # \"params\" is flattened to a list of arrays\n",
    "    shapes = [param.shape for param in params] # shape of each array in the \"params\" list\n",
    "    return np.concatenate([param.reshape(-1) for param in params]), tree, shapes # concat to single 1D array\n",
    "\n",
    "def unconcat_params(params, tree, shapes): # unflatten the parameters\n",
    "    split_vec = np.split(params, onp.cumsum([onp.prod(shape) for shape in shapes])) # \"np.cumsum\" figures out the boundaries where to split the flattened \"params\"\n",
    "    split_vec = [vec.reshape(*shape) for vec, shape in zip(split_vec, shapes)] # reshape slices of vector (\"*\" unpack the tuple into individual arguments)\n",
    "    return jax.tree_util.tree_unflatten(tree, split_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3186e8b",
   "metadata": {},
   "source": [
    "# Evaluation Points & Ground Truth Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d76850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation points\n",
    "with open('2D_Transient_Heat_eval_points.json', 'r') as f:\n",
    "    data_points = json.load(f)\n",
    "\n",
    "# Evaluation coordinates & time\n",
    "mesh_coords = data_points[\"mesh_coord\"][\"0\"]\n",
    "dt_coords = data_points[\"dt_coord\"][\"0\"] # [[0.0], [0.1], ..., [1.0]]\n",
    "times = [dt_coord[0] for dt_coord in dt_coords]  # unpack to [0.0, 0.1, ...]\n",
    "\n",
    "# Load evaluation solutions (by Ground Truth FEM)\n",
    "with open('2D_Transient_Heat_eval_solutions.json', 'r') as f:\n",
    "    data_sol = json.load(f)\n",
    "\n",
    "# Ground truth soution (100 x 100 cells)\n",
    "u_true = np.array(data_sol) # shape: (n_times, n_points)\n",
    "\n",
    "print(\"Evaluation points: \", np.array(mesh_coords).shape)\n",
    "print(\"Evaluation times: \", np.array(dt_coords).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803a110",
   "metadata": {},
   "source": [
    "# PINNs Approximate Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44187f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers for the results\n",
    "y_results, times_adam, times_lbfgs, times_total, times_eval, l2_rel, var, arch\\\n",
    "    =  dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({})\n",
    "# y_results = []  # Initialize y_results as a list\n",
    "# times_adam, times_lbfgs, times_total, times_eval, l2_rel, var, arch = [], [], [], [], [], [], []\n",
    "\n",
    "count = 0 # architecture index\n",
    "for architecture in architecture_list:\n",
    "    times_adam_temp = [] # containers for 10 times training results\n",
    "    times_lbfgs_temp = []\n",
    "    times_total_temp = []\n",
    "    times_eval_temp = []\n",
    "    l2_errors = []\n",
    "    l2_errors_total = []\n",
    "    for i in range(10): # loop over 10 training runs\n",
    "\n",
    "        # Initialize model\n",
    "        model = PDESolution(architecture)\n",
    "        key, key2 = jax.random.split(jax.random.PRNGKey(0)) # create two keys for independent use\n",
    "        batch_dim = 4 # it is just for parameter initialization (can be any value)\n",
    "        feature_dim = 3 # dimension of input point (t, x, y)\n",
    "        params = model.init(key2, np.ones((batch_dim, feature_dim))) # params(dict) : weights & biases initialized randomly\n",
    "\n",
    "        # Initialize Adam optimizer\n",
    "        adam = optax.adam(lr)\n",
    "        opt_state = adam.init(params)\n",
    "\n",
    "        # Training with Adam optimiser\n",
    "        start_time = time.time()\n",
    "        losses, params, opt_state, key, loss_val = jax.block_until_ready(train_loop(params, adam, opt_state, key))\n",
    "        adam_time = time.time() - start_time\n",
    "        times_adam_temp.append(adam_time)\n",
    "        print(\"Adam training time : %.3f secs\" % adam_time)\n",
    "\n",
    "        # Generate collocation points\n",
    "        lb = np.array([0., 0., 0.])\n",
    "        ub = np.array([1., 1., 1.])\n",
    "        domain_points = lb + (ub - lb) * lhs(3, 5000)\n",
    "        boundary_points = lb + (ub - lb) * lhs(3, 100)\n",
    "        init_points = lb[1:] + (ub[1:] - lb[1:]) * lhs(2, 100)\n",
    "        init_point, tree, shapes = concat_params(params)\n",
    "\n",
    "        # Training with L-BFGS optimiser\n",
    "        start_time2 = time.time()\n",
    "        results = tfp.optimizer.lbfgs_minimize(jax.value_and_grad(lambda params: pde_residual(unconcat_params(params, tree, shapes), domain_points) +\n",
    "                                                                                 init_residual(u_init, unconcat_params(params, tree, shapes), init_points) +\n",
    "                                                                                 dirichlet_residual_x(unconcat_params(params, tree, shapes), boundary_points) +\n",
    "                                                                                 dirichlet_residual_y(unconcat_params(params, tree, shapes), boundary_points)),\n",
    "                                               init_point,\n",
    "                                               max_iterations=50000,\n",
    "                                               num_correction_pairs=50, # number of past updates to use for the approximation of Hessian inverse\n",
    "                                               f_relative_tolerance=1.0 * np.finfo(float).eps) # stopping criterion\n",
    "        lbfgs_time = time.time() - start_time2\n",
    "        times_lbfgs_temp.append(lbfgs_time)\n",
    "        times_total_temp.append(adam_time + lbfgs_time)\n",
    "        print(\"L-BFGS training time : %.3f secs\" % lbfgs_time)\n",
    "\n",
    "        \n",
    "        tuned_params = unconcat_params(results.position, tree, shapes)\n",
    "\n",
    "        # l2, times_temp, approx, gt_fem, domain_pt = CompareGT.get_FEM_comparison(mesh_coord,dt_coords,FEM,model,tuned_params)\n",
    "        mesh_coords_squeeze = np.asarray(mesh_coords).squeeze()\n",
    "        dom_mesh_ = np.tile(mesh_coords_squeeze, (len(dt_coords), 1)) # (??? need more understand) repeating the dom_mesh, dt_coords_100.shape-times\n",
    "        dom_ts = np.repeat(np.array(dt_coords),len(mesh_coords)) # (??? need more understand) repeating ts, len(mesh_coords)-times\n",
    "        time_space_coords = np.stack((dom_ts,dom_mesh_[:,0],dom_mesh_[:,1]), axis=1)  #stacking them together, meaning for each mesh coordinate we look at every time instance in ts\n",
    "\n",
    "        # Evaluate model\n",
    "        start_time3 = time.time()\n",
    "        u_approx = jax.block_until_ready(model.apply(tuned_params, time_space_coords).squeeze()) # pass \"time_space_coords\" to model\n",
    "        eval_time = time.time() - start_time3\n",
    "        times_eval_temp.append(eval_time)\n",
    "        print(\"Evaluation time : %.3f secs\" % eval_time)\n",
    "\n",
    "        # L2 error\n",
    "        # u_approx = u_approx.reshape(len(dt_coords),len(mesh_coords)) # going back to shape (n_times, n_points)\n",
    "        u_true = u_true.squeeze()\n",
    "        for j in range(len(dt_coords)):\n",
    "            l2 = np.linalg.norm(u_approx[int(j)] - u_true[int(j)])/np.linalg.norm(u_true[int(j)])\n",
    "            l2_errors.append(l2)\n",
    "\n",
    "        # Total L2 error\n",
    "        l2_errors_total.append(np.mean(np.array(l2_errors))) # mean of the L2 errors over all time steps\n",
    "\n",
    "        print(f'Architecture {architecture}, RUN {i}')\n",
    "        \n",
    "    print('Average Training Time : ', onp.mean(times_total_temp))\n",
    "    print('Average Evaluation Time : ', onp.mean(times_eval_temp))\n",
    "    print('Average Accuracy : ', onp.mean(np.array(l2_errors_total)).tolist())\n",
    "    \n",
    "    y_gt = u_true.tolist()\n",
    "    domain_pts = time_space_coords.tolist()\n",
    "    y_results[count] = u_approx.tolist()\n",
    "    times_adam[count] = onp.mean(times_adam_temp)\n",
    "    times_lbfgs[count] = onp.mean(times_lbfgs_temp)\n",
    "    times_total[count] = onp.mean(times_total_temp)\n",
    "    times_eval[count] = onp.mean(times_eval_temp)\n",
    "    l2_rel[count] = onp.mean(np.array(l2_errors_total)).tolist()\n",
    "    var[count] = onp.var(np.array(l2_errors_total)).tolist()\n",
    "    arch[count] = architecture_list[count]\n",
    "    count += 1\n",
    "    # y_results.append(u_approx.tolist()) # Append to the list\n",
    "    # Append the calculated values for the current architecture to the respective lists\n",
    "    # times_adam.append(onp.mean(times_adam_temp))\n",
    "    # times_lbfgs.append(onp.mean(times_lbfgs_temp))\n",
    "    # times_total.append(onp.mean(times_total_temp))\n",
    "    # times_eval.append(onp.mean(times_eval_temp))\n",
    "    # l2_rel.append(onp.mean(np.array(l2_errors_total)).tolist())\n",
    "    # var.append(onp.var(np.array(l2_errors_total)).tolist())\n",
    "    # arch.append(architecture) # Append the architecture\n",
    "\n",
    "    results = dict({'domain_pts': domain_pts,\n",
    "                    'y_results': y_results,\n",
    "                    'y_gt': y_gt})\n",
    "\n",
    "    evaluation = dict({'arch': arch,\n",
    "                       'times_adam': times_adam,\n",
    "                       'times_lbfgs': times_lbfgs,\n",
    "                       'times_total': times_total,\n",
    "                       'times_eval': times_eval,\n",
    "                       'l2_rel': l2_rel,\n",
    "                       'var': var})\n",
    "\n",
    "    sol_json = 'PINNs_results.json'\n",
    "    with open(sol_json, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    eval_json = 'PINNs_evaluation.json'\n",
    "    with open(eval_json, \"w\") as f:\n",
    "        json.dump(evaluation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28654fe1",
   "metadata": {},
   "source": [
    "# Contour Plot (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation points\n",
    "with open('2D_Transient_Heat_eval_points.json', 'r') as f:\n",
    "    data_points = json.load(f)\n",
    "\n",
    "# Evaluation coordinates & time\n",
    "mesh_coords = data_points[\"mesh_coord\"][\"0\"] # list\n",
    "dt_coords = data_points[\"dt_coord\"][\"0\"] # [[0.0], [0.1], ..., [1.0]]\n",
    "times = [dt_coord[0] for dt_coord in dt_coords]  # unpack to [0.0, 0.1, ...]\n",
    "\n",
    "# Load evaluation results (by PINNs)\n",
    "with open(os.path.join(save_dir, 'PINNs_results.json'), 'r') as f:\n",
    "    data_results = json.load(f)\n",
    "\n",
    "with open(os.path.join(save_dir, 'PINNs_evaluation.json'), 'r') as f:\n",
    "    data_eval = json.load(f)\n",
    "\n",
    "# Slice results data\n",
    "domain_pts = data_results['domain_pts']\n",
    "y_results = data_results['y_results']\n",
    "y_gt = data_results['y_gt']\n",
    "\n",
    "arch = data_eval['arch']\n",
    "times_adam = data_eval['times_adam']\n",
    "times_lbfgs = data_eval['times_lbfgs']\n",
    "times_total = data_eval['times_total']\n",
    "times_eval = data_eval['times_eval']\n",
    "l2_rel = data_eval['l2_rel']\n",
    "var = data_eval['var']\n",
    "\n",
    "# x, y coordinates\n",
    "mesh_coords = np.array(mesh_coords)\n",
    "X = mesh_coords[:, 0]\n",
    "Y = mesh_coords[:, 1]\n",
    "\n",
    "for idx, architecture in arch.items():\n",
    "    # Get the approximate solution for this architecture\n",
    "    u_approx = np.array(y_results[idx])  # (n,)\n",
    "    u_approx = u_approx.reshape(len(dt_coords), len(mesh_coords)) # unsqueeze u_approx\n",
    "\n",
    "    # Contour plot settings\n",
    "    # For a consistent scale across all time steps\n",
    "    u_min = u_approx.min()\n",
    "    u_max = u_approx.max()\n",
    "    # Create n levels between u_min & u_max:\n",
    "    num_levels = 80\n",
    "    levels = np.linspace(u_min, u_max, num_levels)\n",
    "\n",
    "    for t in range(len(times)):\n",
    "\n",
    "        u_approx_t = u_approx[t, :]\n",
    "\n",
    "        fig = plt.figure(figsize=(6, 5))\n",
    "        sc1 = plt.tricontourf(\n",
    "            X, Y, u_approx_t,\n",
    "            levels=levels,\n",
    "            cmap='viridis')\n",
    "        plt.title(f\"PINNs Solution (Architecture={architecture}, t_step={t})\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.colorbar(sc1, shrink=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save figures\n",
    "        fig_dir = f'./fig/sol_contour/PINNs/arch_{idx}'\n",
    "        if not os.path.exists(fig_dir):\n",
    "            os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "        filename = os.path.join(fig_dir, f'sol_{t:04d}.png')\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-fem-pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
