{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Portions of this code are adapted from:\n",
    "#   - https://github.com/TamaraGrossmann/FEM-vs-PINNs.git\n",
    "#   - Grossmann, T. G., Komorowska, U. J., Latz, J., & Schönlieb, C.-B. (2023).\n",
    "#     Can Physics-Informed Neural Networks beat the Finite Element Method?\n",
    "#     arXiv:2302.04107.\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for Google Colab)\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, flax, optax, time, pickle\n",
    "import os\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from functools import partial\n",
    "from pyDOE import lhs\n",
    "from typing import Sequence\n",
    "import json\n",
    "from tensorflow_probability.substrates import jax as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5543116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on the first GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from jax.extend.backend import get_backend\n",
    "print(get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90993946",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d466d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_list = [[20, 20, 20, 1], [100, 100, 100, 1], [500, 500, 500, 1],\n",
    "                     [20, 20, 20, 20, 1], [100, 100, 100, 100, 1], [500, 500, 500, 500, 1],\n",
    "                     [20, 20, 20, 20, 20, 1], [100, 100, 100, 100, 100, 1], [500, 500, 500, 500, 500, 1],\n",
    "                     [20, 20, 20, 20, 20, 20, 1], [100, 100, 100, 100, 100, 100, 1],\n",
    "                     [500, 500, 500, 500, 500, 500, 1], [20, 20, 20, 20, 20, 20, 20, 1],\n",
    "                     [100, 100, 100, 100, 100, 100, 100, 1],] # NN architecture list\n",
    "lr = 1e-4 # learning rate\n",
    "num_epochs = 50000 # number of training epochs\n",
    "eps = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af130ab8",
   "metadata": {},
   "source": [
    "# NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN architecture\n",
    "class PDESolution(flax.linen.Module): # inherit from Module class\n",
    "    features: Sequence[int] # dataclass (e.g. [10, 20, 1])\n",
    "\n",
    "    @flax.linen.compact # a decorator to define the model in more concise and readable way\n",
    "    def __call__(self, x): # __call__: makes an object callable, which enables you to use instances of the class like functions\n",
    "        for feature in self.features[:-1]:\n",
    "            x = flax.linen.tanh(flax.linen.Dense(feature)(x)) # initialize weights(parameters) w/ default initializer\n",
    "        # Final Dense layer\n",
    "        x = flax.linen.Dense(self.features[-1], kernel_init = flax.linen.initializers.glorot_uniform())(x) # initialize weights(parameters) w/ Glorot uniform for stable training\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e3586",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessian-vector product\n",
    "# (it is more general approach than gradient, even if it doesn't make a change in this 1D problem)\n",
    "def hvp(f, primals, tangents):\n",
    "    return jax.jvp(jax.grad(lambda x: f(x)[0]), primals, tangents)[1]\n",
    "\n",
    "# PDE residual\n",
    "@partial(jax.vmap, in_axes = (None, 0, 0, None), out_axes = 0)\n",
    "@partial(jax.jit, static_argnums = (0,))\n",
    "def residual(u, t, x, eps):\n",
    "    u_t = jax.jvp(u, (t, x), (1., 0.))[1] # partial derivative w.r.t t = directional derivative along <1, 0>\n",
    "    u_xx = jax.hessian(u, argnums = 1)(t, x) # differentiate w.r.t argument 1(x)\n",
    "    return u_t - eps*u_xx + (2/eps)*u(t,x)*(1-u(t,x))*(1-2*u(t,x))\n",
    "\n",
    "# Inital condition\n",
    "@partial(jax.vmap, in_axes=0) # vectorized over \"xs\"\n",
    "def u_init(xs):\n",
    "    return np.array([0.25*(np.sin(2*np.pi*xs) + 0.25*np.sin(16*np.pi*xs)) + 0.5])\n",
    "\n",
    "# Loss functionals\n",
    "@jax.jit\n",
    "def pde_residual(params, points):\n",
    "    return np.mean(residual(lambda t, x: model.apply(params, np.stack((x))), points[:, 0], points[:, 1], eps) ** 2) # Mean Squared Error\n",
    "\n",
    "@jax.jit(jax.jit, static_argnums=0)\n",
    "def init_residual(u_init, params, xs):\n",
    "    lhs = model.apply(params, np.stack((np.zeros_like(xs[:, 0]), xs[:, 0]), axis=1))\n",
    "    rhs = u_init(xs[:, 0])\n",
    "    return np.mean((lhs - rhs) ** 2)\n",
    "\n",
    "@jax.jit\n",
    "def boundary_residual(params, ts): # u(t, 0) = u(t, 1)\n",
    "    return np.mean((model.apply(params, np.stack((ts[:, 0], np.zeros_like(ts[:, 0])), axis = 1)) -\n",
    "                    model.apply(params, np.stack((ts[:, 0], np.ones_like(ts[:, 0])), axis = 1)))**2) # ts : (n, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d74a6",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b823ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Step\n",
    "@partial(jax.jit, static_argnums = (1,))\n",
    "def training_step_ini(params, opt, opt_state, key):\n",
    "    \"\"\"\n",
    "    In the PDE problem with initial condition, training NN with I.C. first can be helpful because\n",
    "    - it does not need so many training steps(epochs). (initial guess is already close to the solution)\n",
    "    - It stabilizes subsequent training, because the solution is already near the correct initial profile.\n",
    "    \n",
    "    Args:\n",
    "        params: model parameters\n",
    "        opt: optimizer\n",
    "        opt_state: optimizer state\n",
    "        key: random key for sampling\n",
    "    \"\"\"\n",
    "    lb = onp.array([0., 0.]) # lower bound\n",
    "    ub = onp.array([0.05, 1.]) # upper bound\n",
    "    # scale the samples from [0, 1] to [lb, ub]\n",
    "    domain_points = lb + (ub - lb) * lhs(2, 20000) # latin hypercube sampling 20000 points within (ti, xi) ∈ [0, 0.05] × [0, 1]\n",
    "    boundary_points = lb[0] + (ub[0] - lb[0]) * lhs(1, 250) # latin hypercube sampling 250 points within ti ∈ [0, 0.05]\n",
    "    init_points = lb[1] + (ub[1] - lb[1]) * lhs(1, 500) # latin hypercube sampling 500 points within xi ∈ [0, 1]\n",
    "\n",
    "    loss_val, grad = jax.value_and_grad(lambda params: init_residual(u_init,params, init_points))(params)\n",
    "    update, opt_state = opt.update(grad, opt_state, params) # update using \"grad\"\n",
    "    params = optax.apply_updates(params, update) # apply updates to \"params\"\n",
    "    return params, opt_state, key, loss_val\n",
    "\n",
    "@partial(jax.jit, static_argnums = (1,))\n",
    "def training_step(params, opt, opt_state, key):\n",
    "    # (same as above)\n",
    "    lb = onp.array([0., 0.])\n",
    "    ub = onp.array([0.05, 1.])\n",
    "    domain_points = lb + (ub - lb) * lhs(2, 20000)\n",
    "    boundary_points = lb[0] + (ub[0] - lb[0]) * lhs(1, 250)\n",
    "    init_points = lb[1] + (ub[1] - lb[1]) * lhs(1, 500)\n",
    "\n",
    "    # Weight factor 1000 for I.C. loss term heuristically gives the best result\n",
    "    # (total loss will be more sensitive to I.C. loss than other loss terms)\n",
    "    loss_val, grad = jax.value_and_grad(lambda params: pde_residual(params, domain_points) + \n",
    "                                        1000 * init_residual(u_init,params, init_points) +\n",
    "                                        boundary_residual(params, boundary_points))(params)\n",
    "    update, opt_state = opt.update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, update)\n",
    "    return params, opt_state, key, loss_val\n",
    "\n",
    "# Training loop\n",
    "def train_loop(params, adam, opt_state, key):\n",
    "    losses = []\n",
    "    for i in range(7000):\n",
    "        params, opt_state, key, loss_val = training_step_ini(params, adam, opt_state, key)\n",
    "        losses.append(loss_val.item())\n",
    "    \n",
    "    for _ in range(num_epochs): # \"_\" is used because the variable is not used in for loop\n",
    "        params, opt_state, key, loss_val = training_step(params, adam, opt_state, key)\n",
    "        losses.append(loss_val.item())\n",
    "    return losses, params, opt_state, key, loss_val # return final values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc034fa7",
   "metadata": {},
   "source": [
    "# Helper Functions for L-BFGS Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS requires the parameters to be a single flattened array!\n",
    "def concat_params(params): # flatten the parameters\n",
    "    params, tree = jax.tree_util.tree_flatten(params) # \"params\" is flattened to a list of arrays\n",
    "    # \"tree\" describes the original structure of parameters. It allows to reconstruct the original nested format later.\n",
    "    shapes = [param.shape for param in params] # shape of each array in the \"params\" list\n",
    "    return np.concatenate([param.reshape(-1) for param in params]), tree, shapes # concat to single 1D array\n",
    "\n",
    "def unconcat_params(params, tree, shapes): # unflatten the parameters\n",
    "    split_vec = np.split(params, np.cumsum([np.prod(shape) for shape in shapes])) # \"np.cumsum\" figures out the boundaries where to split the flattened \"params\"\n",
    "    split_vec = [vec.reshape(*shape) for vec, shape in zip(split_vec, shapes)] # reshape slices of vector (\"*\" unpack the tuple into individual arguments)\n",
    "    return jax.tree_util.tree_unflatten(tree, split_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3186e8b",
   "metadata": {},
   "source": [
    "# Load Ground Truth Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d76850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8803a110",
   "metadata": {},
   "source": [
    "# Train PINN & Approximate Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-fem-pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
