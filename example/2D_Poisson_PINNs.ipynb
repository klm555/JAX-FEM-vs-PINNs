{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Portions of this code are adapted from:\n",
    "#   - https://github.com/TamaraGrossmann/FEM-vs-PINNs.git\n",
    "#   - Grossmann, T. G., Komorowska, U. J., Latz, J., & Sch√∂nlieb, C.-B. (2023).\n",
    "#     Can Physics-Informed Neural Networks beat the Finite Element Method?\n",
    "#     arXiv:2302.04107.\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b792e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for Google Colab)\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, flax, optax, time, pickle\n",
    "import os\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from functools import partial\n",
    "from pyDOE import lhs\n",
    "from typing import Sequence\n",
    "import json\n",
    "from tensorflow_probability.substrates import jax as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359462d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must set CUDA_VISIBLE_DEVICES before importing JAX or any other library that initializes GPUs.\n",
    "# Otherwise, the environment variable change might be ignored.\n",
    "# \"0, 1\": first two GPUs / \"\": no GPU (CPU instead)\n",
    "\n",
    "# Run on the first GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from jax.extend.backend import get_backend\n",
    "print(get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4395d8",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_list = [[20, 1], [60, 1], [20, 20, 1], [60, 60, 1], \n",
    "                     [20, 20, 20, 1], [60, 60, 60, 1],\n",
    "                     [20, 20, 20, 20, 1], [60, 60, 60, 60, 1],\n",
    "                     [20, 20, 20, 20, 20, 1], [60, 60, 60, 60, 60, 1],\n",
    "                     [120, 120, 120, 120, 120, 1]] # NN architecture list\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 20000 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd6f92",
   "metadata": {},
   "source": [
    "# NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN architecture\n",
    "class PDESolution(flax.linen.Module): # inherit from Module class\n",
    "    features: Sequence[int] # dataclass (e.g. [10, 20, 1])\n",
    "\n",
    "    @flax.linen.compact # a decorator to define the model in more concise and readable way\n",
    "    def __call__(self, x): # __call__: makes an object callable, which enables you to use instances of the class like functions\n",
    "        for feature in self.features[:-1]:\n",
    "            x = flax.linen.tanh(flax.linen.Dense(feature)(x))\n",
    "        # Final Dense layer\n",
    "        x = flax.linen.Dense(self.features[-1])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bef246",
   "metadata": {},
   "source": [
    "# Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=(0, 0), out_axes=0)\n",
    "@jax.jit\n",
    "def analytic_sol(xs,ys):\n",
    "    sol = (xs**2) * (xs-1)**2 * ys * (ys-1)**2\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcc721",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b144380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives for the Neumann B.C.\n",
    "@partial(jax.vmap, in_axes=(None, 0, 0), out_axes=(0, 0, 0))\n",
    "@jax.jit\n",
    "def neumann_derivatives(params, xs, ys):\n",
    "    u = lambda x, y: model.apply(params, np.stack((x, y)))\n",
    "    du_dx_0 = jax.jvp(u, (0., ys), (1., 0.))[1] # du/dx(0, ys)\n",
    "    du_dx_1 = jax.jvp(u, (1., ys), (1., 0.))[1] # du/dx(1, ys)\n",
    "    du_dy_1 = jax.jvp(u, (xs, 1.), (0., 1.))[1] # du/dy(xs, 1)\n",
    "    return du_dx_0, du_dx_1, du_dy_1\n",
    "\n",
    "# PDE residual\n",
    "@partial(jax.vmap, in_axes = (None, 0, 0), out_axes = 0)\n",
    "@partial(jax.jit, static_argnums = (0,)) # decorator closest to the function is applied first\n",
    "def residual(u, x, y):\n",
    "    H1 = jax.hessian(u, argnums=0)(x, y) # equivalent to \"hvp\" in 1D_Poisson_PINNs code\n",
    "    H2 = jax.hessian(u, argnums=1)(x, y)\n",
    "    lhs = H1 + H2\n",
    "    rhs = 2*((x**4)*(3*y-2) + (x**3)*(4-6*y) + (x**2)*(6*(y**3)-12*(y**2)+9*y-2) - 6*x*((y-1)**2)*y + ((y-1)**2)*y)\n",
    "    return lhs - rhs\n",
    "\n",
    "# Loss functionals\n",
    "@jax.jit\n",
    "def pde_residual(params, points):\n",
    "    return np.mean(residual(lambda x, y: model.apply(params, np.stack((x, y))), points[:, 0], points[:, 1]) ** 2) # Mean Squared Error\n",
    "\n",
    "@jax.jit\n",
    "def dirichlet_residual(params, points):\n",
    "    return np.mean((model.apply(params, np.stack((points[:,0], np.zeros_like(points[:,1])), axis=1))) ** 2)\n",
    "\n",
    "@partial(jax.jit, static_argnums=0) # du/dx(0,y) = 0, du/dx(1,y) = 0, du/dy(x,1) = 0\n",
    "def neumann_residual(neumann_derivatives, params, points):\n",
    "    du_dx_0, du_dx_1, du_dy_1 = neumann_derivatives(params, points[:, 0], points[:, 1])\n",
    "    return np.mean((du_dx_0**2) + (du_dx_1**2) + (du_dy_1**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20522bac",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Step\n",
    "@partial(jax.jit, static_argnums = (1, 4))\n",
    "def training_step(params, opt, opt_state, key, neumann_derivatives):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params: model parameters\n",
    "        opt: optimizer\n",
    "        opt_state: optimizer state\n",
    "        key: random key for sampling\n",
    "    \"\"\"\n",
    "    lb = np.array([0., 0.]) # lower bound\n",
    "    ub = np.array([1., 1.]) # upper bound\n",
    "    domain_points = lb + (ub - lb) * lhs(2, 2000) # latin hypercube sampling 256 points within [0, 1]\n",
    "    boundary_points = lb + (ub - lb) * lhs(2, 250) # scaless the samples from [0, 1] to [lb, ub]\n",
    "\n",
    "    loss_pde = pde_residual(params, domain_points)\n",
    "    loss_dirichlet = dirichlet_residual(params, boundary_points)\n",
    "    loss_neumann = neumann_residual(neumann_derivatives, params, boundary_points)\n",
    "\n",
    "    loss_val, grad = jax.value_and_grad(lambda params: loss_pde + loss_dirichlet + loss_neumann)(params)\n",
    "    \n",
    "    update, opt_state = opt.update(grad, opt_state, params) # update using \"grad\"\n",
    "    params = optax.apply_updates(params, update) # apply updates to \"params\"\n",
    "    return params, opt_state, key, loss_val\n",
    "\n",
    "# Training loop\n",
    "def train_loop(params, adam, opt_state, key, neumann_derivatives):\n",
    "    losses = []\n",
    "    for _ in range(num_epochs): # \"_\" is used because the variable is not used in for loop\n",
    "        params, opt_state, key, loss_val = training_step(params, adam, opt_state, key, neumann_derivatives)\n",
    "        losses.append(loss_val.item())\n",
    "    return losses, params, opt_state, key, loss_val # return final values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c67af",
   "metadata": {},
   "source": [
    "# Train PINN & Approximate Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0323f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers for the results\n",
    "y_results, domain_pts, times_adam, times_lbfgs, times_total, times_eval, l2_rel, var, arch\\\n",
    "    = dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({})\n",
    "\n",
    "count = 0 # architecture index\n",
    "for architecture in architecture_list:\n",
    "    print('Architecture : %s' %architecture)\n",
    "    times_adam_temp = [] # containers for 10 times training results\n",
    "    times_lbfgs_temp = []\n",
    "    times_total_temp = []\n",
    "    times_eval_temp = []\n",
    "    accuracy_temp = []\n",
    "    for _ in range(10): # loop over 10 training runs\n",
    "        # Initialize Model\n",
    "        model = PDESolution(architecture)\n",
    "        key, key2 = jax.random.split(jax.random.PRNGKey(0)) # create two keys for independent use\n",
    "        batch_dim = 8 # it is just for parameter initialization (can be any value)\n",
    "        feature_dim = 2 # dimension of input point (x coord)\n",
    "        params = model.init(key, np.ones((batch_dim, feature_dim))) # params(dict) : weights and biases initialized randomly\n",
    "\n",
    "        # Initialize Optimizer\n",
    "        adam = optax.adam(learning_rate = lr) #\n",
    "        opt_state = adam.init(params) # opt_state : internal states of the Adam optimizer\n",
    "\n",
    "        # Start Training with Adam Optimizer\n",
    "        start_time = time.time()\n",
    "        losses, params, opt_state, key, loss_val = jax.block_until_ready(train_loop(params, adam, opt_state, key, neumann_derivatives))\n",
    "        adam_time = time.time() - start_time\n",
    "        times_adam_temp.append(adam_time)\n",
    "        print('Adam Training Time : %f secs' %adam_time)\n",
    "\n",
    "        # Generate data\n",
    "        lb = onp.array([0., 0.])\n",
    "        ub = onp.array([1., 1.])\n",
    "        domain_points = lb + (ub - lb) * lhs(2, 2000)\n",
    "        boundary_points = lb + (ub - lb) * lhs(2, 250)\n",
    "\n",
    "        init_point, tree, shapes = concat_params(params)\n",
    "\n",
    "        # L-BFGS Optimization\n",
    "        print('Starting L-BFGS Optimization')\n",
    "        start_time2 = time.time()\n",
    "        results = tfp.optimizer.lbfgs_minimize(jax.value_and_grad(lambda params:\n",
    "                                                                  pde_residual(unconcat_params(params, tree, shapes), domain_points) +\n",
    "                                                                  dirichlet_residual(unconcat_params(params, tree, shapes), boundary_points) +\n",
    "                                                                  neumann_residual(neumann_derivatives, unconcat_params(params, tree, shapes), boundary_points)),\n",
    "                                               init_point, max_iterations = 50000,\n",
    "                                               num_correction_pairs = 50, # number of past updates to use for the approximation of the Hessian inverse.\n",
    "                                               f_relative_tolerance = 1.0*np.finfo(float).eps) # stopping criterion\n",
    "        lbfgs_time = time.time() - start_time2\n",
    "        times_lbfgs_temp.append(lbfgs_time)\n",
    "        times_total_temp.append(adam_time + lbfgs_time)\n",
    "\n",
    "        # Comparison to Ground Truth\n",
    "        tuned_params = unconcat_params(results.position, tree, shapes)\n",
    "\n",
    "        with open('2D_Poisson_eval_points.json', 'r') as f:\n",
    "            domain_points = json.load(f) # pre-specified evaluation points (different from training points) for measuring error.\n",
    "            domain_points = np.array(domain_points)\n",
    "\n",
    "        start_time3 = time.time()\n",
    "        u_approx = jax.block_until_ready(model.apply(tuned_params, np.stack((domain_points[:, 0], domain_points[:, 1]), axis=1)).squeeze()) # pass the \"domain_points\" to the trained model\n",
    "        eval_time = time.time() - start_time3\n",
    "        times_eval_temp.append(eval_time)\n",
    "\n",
    "        u_true = analytic_sol(domain_points[:,0],domain_points[:,1]).squeeze() # ground truth\n",
    "        run_accuracy = (onp.linalg.norm(u_approx - u_true)) / onp.linalg.norm(u_true) # relative L2 error\n",
    "        accuracy_temp.append(run_accuracy)\n",
    "\n",
    "    y_gt = u_true.tolist() # for storing into dict\n",
    "    y_results[count] = u_approx.tolist()\n",
    "    domain_pts[count] = domain_points.tolist()\n",
    "    times_adam[count] = onp.mean(times_adam_temp) # mean times across the 10 runs\n",
    "    times_lbfgs[count] = onp.mean(times_lbfgs_temp)\n",
    "    times_total[count] = onp.mean(times_total_temp)\n",
    "    times_eval[count] = onp.mean(times_eval_temp)\n",
    "    l2_rel[count] = onp.mean(accuracy_temp).tolist()\n",
    "    var[count] = onp.var(accuracy_temp).tolist() # variance of the error across the 10 runs\n",
    "    arch[count] = architecture_list[count]\n",
    "    count += 1\n",
    "\n",
    "    results = dict({'domain_pts': domain_pts,\n",
    "                    'y_results': y_results,\n",
    "                    'y_gt': y_gt})\n",
    "\n",
    "    evaluation = dict({'arch': arch,\n",
    "                    'times_adam': times_adam,\n",
    "                    'times_lbfgs': times_lbfgs,\n",
    "                    'times_total': times_total,\n",
    "                    'times_eval': times_eval,\n",
    "                    'l2_rel': l2_rel,\n",
    "                    'var': var})\n",
    "\n",
    "    # Save Results & Evaluation\n",
    "    save_dir = './2D_Poisson'\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'PINNs_results.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'PINNs_evaluation.json'), 'w') as f:\n",
    "        json.dump(evaluation, f)\n",
    "\n",
    "    print(json.dumps(evaluation, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf67ca",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df31838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "with open(os.path.join(save_dir, 'PINNs_results.json'), 'r') as f:\n",
    "    data_results = json.load(f)\n",
    "    \n",
    "with open(os.path.join(save_dir, 'PINNs_evaluation.json'), 'r') as f:\n",
    "    data_eval = json.load(f)\n",
    "\n",
    "\n",
    "# Pick one architecture\n",
    "arch_idx = '8'\n",
    "arch_indices = sorted([int(k) for k in data_eval['arch'].keys()])\n",
    "arch_indices_str = [str(i) for i in arch_indices]\n",
    "# For instance, pick the first architecture index in data_results['y_results']\n",
    "# example_key = list(data_results['y_results'].keys())[0]  # first architecture\n",
    "domain_points = np.array(data_results['domain_pts'][arch_idx])  # shape (N, 2)\n",
    "u_approx   = np.array(data_results['y_results'][arch_idx])  # shape (N,)\n",
    "u_exact    = np.array(data_results['y_gt'])          # shape (N,)\n",
    "\n",
    "# Unpack X, Y\n",
    "X = domain_points[:, 0]\n",
    "Y = domain_points[:, 1]\n",
    "\n",
    "# Approximate solution (left)\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "sc1 = ax1.scatter(X, Y, u_approx, c=u_approx, cmap='viridis')\n",
    "sc1 = ax1.tricontourf(\n",
    "    X, Y, u_approx, \n",
    "    levels=50, \n",
    "    cmap='viridis'\n",
    ")\n",
    "ax1.set_title(f\"PINN Approx. Solution (Arch index={arch_idx})\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "fig.colorbar(sc1, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Exact solution (right)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "sc2 = ax2.tricontourf(\n",
    "    X, Y, u_exact, \n",
    "    levels=50, \n",
    "    cmap='viridis'\n",
    ")\n",
    "ax2.set_title(\"Exact Solution\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "fig.colorbar(sc2, ax=ax2, shrink=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D_Poisson_PINNs_approx_vs_exact.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Relative Error vs. Training Time or Evaluation Time\n",
    "train_times = []\n",
    "eval_times  = []\n",
    "rel_errors  = []\n",
    "arch_labels = []\n",
    "\n",
    "for i in arch_indices_str:\n",
    "    train_times.append(data_eval['times_total'][i])  # or 'times_adam'[i], 'times_lbfgs'[i], etc.\n",
    "    eval_times.append(data_eval['times_eval'][i])\n",
    "    rel_errors.append(data_eval['l2_rel'][i])\n",
    "    arch_labels.append(str(data_eval['arch'][i]))  # e.g. \"20,20,1\"\n",
    "\n",
    "# Relative error vs. total training time\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(rel_errors, train_times, color='blue')\n",
    "for idx, label in enumerate(arch_labels):\n",
    "    plt.text(rel_errors[idx], train_times[idx], label, fontsize=8,\n",
    "             ha='left', va='bottom')\n",
    "\n",
    "plt.xlabel(r\"Relative $L_2$ Error\")\n",
    "plt.ylabel(\"Total Training Time (seconds)\")\n",
    "plt.title(\"Relative Error vs. Total Training Time (by architecture)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D_Poisson_PINNs_L2rel_vs_Train_time.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Relative error vs. evaluation time\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(rel_errors, eval_times, color='red')\n",
    "for idx, label in enumerate(arch_labels):\n",
    "    plt.text(rel_errors[idx], eval_times[idx], label, fontsize=8,\n",
    "             ha='left', va='bottom')\n",
    "\n",
    "plt.xlabel(r\"Relative $L_2$ Error\")\n",
    "plt.ylabel(\"Evaluation Time (seconds)\")\n",
    "plt.title(\"Relative Error vs. Evaluation Time (by architecture)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D_Poisson_PINNs_L2rel_vs_Eval_time.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-fem-pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
